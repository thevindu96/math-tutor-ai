To understand a finite Markov Decision Process (MDP) in the context of a 3x3 grid, where the agent is trying to reach a goal state, let's break down the components of an MDP and then apply them to your example. ### Components of an MDP 1. **States (S):** The set of all possible states. In a 3x3 grid, each cell can be a state. So, there are 9 states in total. 2. **Actions (A):** The set of actions available to the agent. Common actions in a grid world are Up, Down, Left, and Right. 3. **Transition Function (T):** This describes the probability of moving from one state to another given an action. In a deterministic environment, taking an action in a state leads to a specific, predictable next state. 4. **Reward Function (R):** This assigns a reward for each state or state-action pair. The goal state might have a high reward, while other states have lower or zero rewards. 5. **Policy (Ï€):** A map from states to a distribution over actions. A stochastic policy means that the agent might choose different actions with certain probabilities even in the same state. ### Example: 3x3 Grid MDP Imagine a 3x3 grid where the top-left corner is state \( s_1 \) and the bottom-right corner is the goal state \( s_9 \). The agent starts at a random position and aims to reach the goal state. #### States Let's label the states as follows: \[ \begin{array}{ccc} s_1 & s_2 & s_3 \\ s_4 & s_5 & s_6 \\ s_7 & s_8 & s_9 \\ \end{array} \] #### Actions The available actions are: - \( \text{Up} \) - \( \text{Down} \) - \( \text{Left} \) - \( \text{Right} \) #### Transition Function Since the environment is deterministic, each action deterministically moves the agent to the next state, if possible. For instance: - If the agent is in \( s_1 \) and takes the action "Right," it moves to \( s_2 \). - If the agent is in \( s_1 \) and takes the action "Down," it moves to \( s_4 \). Boundary conditions prevent moving outside the grid (e.g., moving "Up" from \( s_1 \) keeps the agent in \( s_1 \)). #### Reward Function A simple reward function could be: - Reward of 0 for all states except the goal state. - Reward of +1 for reaching the goal state \( s_9 \). #### Stochastic Policy A stochastic policy might look like this: \[ \pi(a | s) = \begin{cases} 0.8 & \text{if } a \text{ is the optimal action leading closer to the goal} \\ 0.2 & \text{otherwise, for any other action} \end{cases} \] ### Finding the Ideal Policy To find the ideal policy, you can use methods such as Value Iteration or Policy Iteration. These methods aim to maximize the expected cumulative reward, which often involves finding the shortest path to the goal in a grid. #### Value Iteration Algorithm 1. **Initialize:** Set \( V(s) = 0 \) for all states \( s \). 2. **Iterate:** For each state \( s \), update \( V(s) \) using the Bellman equation: \[ V(s) = \max_{a} \left( R(s, a) + \gamma \sum_{s'} T(s' | s, a) V(s') \right) \] where \( \gamma \) is the discount factor, typically between 0 and 1. 3. **Policy Extraction:** Once \( V(s) \) is stable, extract the policy \( \pi(s) \) as: \[ \pi(s) = \arg\max_{a} \left( R(s, a) + \gamma \sum_{s'} T(s' | s, a) V(s') \right) \] This process will help you find the optimal policy that guides the agent to the goal state efficiently. ### Conclusion In a 3x3 grid MDP with deterministic transitions and stochastic policies, the goal is to determine the policy that maximizes the expected reward by guiding the agent to the goal efficiently. The value iteration method is a powerful tool for finding such an optimal policy.